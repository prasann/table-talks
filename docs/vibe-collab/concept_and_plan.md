# ğŸ§  TableTalk: Concept & Implementation Plan

## ğŸ” Problem Statement

Our datasets are spread across multiple files (CSV, Parquet, etc.), each representing different tables. Manually inspecting their schemas, understanding commonalities, detecting inconsistencies, and exploring anomalies is time-consuming.

**Solution**: A **Python-based CLI assistant**, backed by a **local Small Language Model (SLM)**, that enables **natural language exploration of data schemas**.

---

## ğŸ¯ Core Concept & Goals

### Vision
A **schema-aware, conversational EDA system** running fully locally with intelligent query understanding and multi-step analysis capabilities.

### Key Goals
- Extract and store schema metadata across files
- Identify schema mismatches, commonalities, and data quality issues
- Enable **natural language questions** via CLI chat with LLM-powered understanding
- Support **complex multi-step analysis** through intelligent tool orchestration
- **Local-first processing** with privacy and cost control
- **Graceful fallback** to basic mode when LLM unavailable

---

## ğŸ—ï¸ Architecture Overview

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   CLI Interface    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚    LLM Agent       â”‚  â† Orchestrates everything
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Context Manager    â”‚  â† LLM parsing + regex fallback
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Schema Tools         â”‚  â† 8 specialized functions
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Metadata Store (DuckDB)      â”‚  â† schema_info table
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§° Component Details

### 1. **Context Manager** (Intelligence Layer)
**Dual-mode query processing:**
- **LLM Mode**: Complex query parsing, multi-step planning, response synthesis
- **Basic Mode**: Regex pattern matching for simple queries

```python
# Example LLM parsing
"Find data quality issues and suggest fixes" â†’ 
{
  "steps": [
    {"tool": "detect_type_mismatches"},
    {"tool": "detect_semantic_type_issues"}, 
    {"tool": "detect_column_name_variations"}
  ],
  "synthesis": "Combine findings with recommendations"
}
```

### 2. **Schema Tools** (8 Functions)
- `get_file_schema(file_name)` - Detailed schema for specific file
- `list_files()` - All scanned files with statistics
- `find_columns(column_name)` - Files containing specific columns
- `detect_type_mismatches()` - Same column, different types
- `find_common_columns()` - Shared columns across files
- `database_summary()` - Overall statistics
- `detect_semantic_type_issues()` - Type vs naming mismatches
- `detect_column_name_variations()` - Similar names, different conventions

### 3. **Metadata Store** (DuckDB)
```sql
CREATE TABLE schema_info (
  file_name TEXT,
  column_name TEXT,
  data_type TEXT,
  null_count INTEGER,
  unique_count INTEGER,
  total_rows INTEGER,
  file_size_mb REAL,
  last_scanned TIMESTAMP
);
```

### 4. **LLM Integration** (Ollama + Phi-3)
- Local model serving via Ollama
- LangChain integration for tool orchestration
- Intelligent query understanding and response synthesis
- Context-aware follow-up suggestions

---

## ğŸ”„ Query Processing Examples

### Simple Queries (Basic Mode)
```
Input: "What files do we have?"
Processing: Regex â†’ list_files() â†’ Response
Output: "ğŸ“ Found 5 files: orders.csv (8 columns)..."
```

### Complex Queries (LLM Mode)
```
Input: "Find data quality issues and suggest fixes"
Processing: LLM Parse â†’ Multi-tool execution â†’ Synthesis
Steps:
1. detect_type_mismatches() â†’ "user_id: int64 vs string"
2. detect_semantic_type_issues() â†’ "amount stored as text"
3. detect_column_name_variations() â†’ "cust_id vs customer_id"
Output: Synthesized insights + actionable recommendations
```

---

## ğŸš€ Implementation Plan

### Phase 1: Core Infrastructure âœ…
- [x] Project structure and dependencies
- [x] DuckDB metadata storage with schema
- [x] CSV/Parquet schema extraction
- [x] Basic tool functions

### Phase 2: Intelligence Layer âœ…  
- [x] Context manager with LLM integration
- [x] Ollama + Phi-3 setup
- [x] Query parsing (LLM + regex fallback)
- [x] Multi-step analysis capabilities

### Phase 3: Enhanced Features âœ…
- [x] 8 comprehensive schema tools
- [x] Intelligent response synthesis
- [x] Follow-up suggestion generation
- [x] Error handling and graceful degradation

### Phase 4: User Experience âœ…
- [x] CLI interface with status monitoring
- [x] Natural language conversation flow
- [x] Help system and documentation
- [x] Testing and refinement

### Phase 5: Simplification âœ…
- [x] Consolidated architecture (removed "enhanced" complexity)
- [x] Single context manager with dual-mode operation
- [x] Streamlined codebase (~40% reduction)
- [x] Clean documentation

---

## ğŸ¯ Key Features

### Natural Language Understanding
- **Simple queries**: Direct regex pattern matching
- **Complex queries**: LLM-powered semantic parsing
- **Multi-part questions**: Automatic tool orchestration
- **Context awareness**: Previous conversation understanding

### Intelligent Analysis
- **Dynamic tool selection**: Choose optimal analysis approach
- **Multi-step pipelines**: Combine tools for comprehensive insights
- **Response synthesis**: Coherent insights from multiple sources
- **Proactive suggestions**: Intelligent follow-up recommendations

### Local-First Design
- **Privacy**: All processing happens locally
- **Cost control**: No external API calls
- **Reliability**: Works offline
- **Performance**: Fast local inference

### Graceful Degradation
- **LLM optional**: Full functionality without Ollama
- **Progressive enhancement**: Better experience with LLM
- **Error resilience**: Continues working despite failures
- **Clear status**: Users know current capabilities

---

## ğŸ“Š Success Metrics

### Functional Requirements âœ…
- Extract schemas from CSV/Parquet files
- Natural language query processing
- Multi-step analysis for complex questions
- Local LLM integration with fallback

### Performance Requirements âœ…
- Schema extraction: < 5 seconds for 10MB files
- Query response: < 3 seconds typical
- Memory usage: < 500MB for normal datasets
- File support: Up to 100MB per file

### User Experience âœ…
- Intuitive natural language interface
- Clear error messages and guidance
- Helpful follow-up suggestions
- Status monitoring and help system

---

## ğŸ”® Future Extensions

### Short-term Enhancements
- **Advanced file formats**: JSON, XML schema extraction
- **Data profiling**: Statistical analysis and anomaly detection
- **Export capabilities**: Generate data dictionaries and reports
- **Batch processing**: Directory-wide analysis

### Medium-term Features  
- **Web interface**: Streamlit-based dashboard
- **Visualization**: Schema diagrams and data flow charts
- **Collaboration**: Team sharing and annotation
- **Integration**: API endpoints for external tools

### Long-term Vision
- **Embedding search**: Semantic similarity across schemas
- **Code generation**: Auto-generate SQL/Python for analysis
- **ML integration**: Automated data quality scoring
- **Enterprise features**: Role-based access, audit trails

---

## ğŸ› ï¸ Technology Stack

### Core Technologies
- **Python 3.11+**: Primary language
- **DuckDB**: Fast embedded analytics database
- **Pandas/PyArrow**: Data processing and file I/O
- **LangChain**: LLM framework and tool integration

### LLM Stack
- **Ollama**: Local model serving
- **Phi-3**: Microsoft's efficient reasoning model
- **Fallback**: Regex patterns for basic queries

### Development Tools
- **pytest**: Testing framework
- **black/flake8**: Code formatting and linting
- **mypy**: Type checking
- **YAML**: Configuration management

---

## ğŸ‰ Current Status

**TableTalk is fully implemented and functional!**

### âœ… Completed Features
- Intelligent query understanding (LLM + fallback)
- 8 comprehensive schema analysis tools
- Multi-step analysis with response synthesis
- Clean CLI interface with status monitoring
- Local-first processing with Ollama integration
- Comprehensive error handling and graceful degradation
- Simplified, maintainable architecture

### ğŸ§ª Ready for Testing
```bash
# Quick start
python src/main.py
/scan data/sample
# Then ask natural language questions!
```

### ğŸ“ˆ Impact
- **Developer productivity**: Natural language data exploration
- **Data quality**: Automated schema analysis and issue detection
- **Local privacy**: No external dependencies for core functionality
- **Extensibility**: Clean architecture for future enhancements

---

**TableTalk transforms schema exploration from manual inspection to intelligent conversation!** ğŸ—£ï¸
